{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import *\n",
    "for exp in['events_pred']: #\n",
    "    source_file=f'../dataset/plmarker/test_400window.jsonl'\n",
    "    source_file=f'../dataset/plmarker/test_sent.jsonl'\n",
    "    exp=f'{exp}'\n",
    "    pred_ent_file=f'../dataset/plmarker/predictions/{exp}/ner_pred_results.json'\n",
    "    pred_re_file=f'../dataset/plmarker/predictions/{exp}/re_pred_results.json'\n",
    "    out_file=f'../dataset/plmarker/predictions/{exp}.json'\n",
    "    plmarkerPred2json(source_file,pred_ent_file,pred_re_file,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import *\n",
    "ann_path='../dataset/BRAT/test'\n",
    "\n",
    "\n",
    "pred_file='../dataset/spert/test_spert_sent.json'\n",
    "source_file='../dataset/spert/test_spert_sent.json'\n",
    "out_dir='../dataset/spert/temp'\n",
    "\n",
    "pred_file='../dataset/plmarker/predictions/events_pred.json'\n",
    "source_file='../dataset/spert/test_spert_sent.json'\n",
    "out_dir='../dataset/plmarker/predictions/BRAT'\n",
    "\n",
    "# pred_file='../dataset/plmarker/predictions/400window_preEnt.json'\n",
    "# source_file='../dataset/spert/test_spert_400window.json'\n",
    "# out_dir='../dataset/plmarker/predictions/BRAT_REpreEnt'\n",
    "\n",
    "offset_key,start_key,end_key=\"token_offsets\",'start','end'\n",
    "json2brat(source_file,ann_path,offset_key,start_key,end_key,pred_file,out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from format_helper import *\n",
    "\n",
    "for exp in ['llama2_events']: #'llama80b_events','t5_events','llama8b_events','gpt4_events'\n",
    "    pred_file=f'../dataset/GenQA/predictions/{exp}.json'\n",
    "    if 'llama' in exp or 'gpt' in exp:\n",
    "        pred_file+='l'\n",
    "    ann_path='../dataset/BRAT/test'\n",
    "    out_dir=f'../dataset/GenQA/predictions/{exp}/pred'\n",
    "    ref_dir=f'../dataset/GenQA/predictions/{exp}/ref'\n",
    "    ref_file='../dataset/GenQA/test_events.json'\n",
    "    if 'gpt' in exp:\n",
    "        ref_file='../dataset/GenQA/gpt4_test_events.json'\n",
    "    glmEvents2BRAT(pred_file,ann_path,out_dir,ref_dir,ref_file=ref_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring brat\n",
    "from brat_scoring.scoring import score_brat\n",
    "from format_helper import *\n",
    "\n",
    "for exp in ['llama2_events']: #'llama80b_events',,'llama8b_events''gpt4_events'\n",
    "    out_dir=f'../dataset/GenQA/predictions/{exp}/pred'\n",
    "    ref_dir=f'../dataset/GenQA/predictions/{exp}/ref'\n",
    "    score_dir=f'../dataset/GenQA/predictions/{exp}/'\n",
    "\n",
    "exp='plmarker' #brat\n",
    "out_dir=f'../dataset/{exp}/predictions/BRAT'\n",
    "ref_dir=f'../dataset/BRAT/test'\n",
    "score_dir=f'../dataset/{exp}/predictions/BRAT/score'\n",
    "\n",
    "\n",
    "score_brat( \n",
    "    gold_dir = ref_dir,\n",
    "    predict_dir = out_dir,\n",
    "    labeled_args = [CHANGE,ASSERTION,SEVERITY],\n",
    "    score_trig = \"overlap\",\n",
    "    score_span = \"overlap\",\n",
    "    score_labeled = \"label\",\n",
    "    include_detailed = True,\n",
    "    output_path =score_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizing entity performance\n",
    "from glob import glob\n",
    "exps=['plmarker','spert','llama80b_events','llama8b_events','llama2_events','t5_events','gpt4_events']#,'spert']\n",
    "for exp in exps:\n",
    "        scores={\n",
    "        \"Drug,Trigger\":[0,0,0],\n",
    "        \"Problem,Trigger\":[0,0,0],\n",
    "        \"Problem,Assertion\":[0,0,0],\n",
    "        \"Problem,Change\":[0,0,0],\n",
    "        \"Problem,Severity\":[0,0,0],\n",
    "        \"Problem,Anatomy\":[0,0,0],\n",
    "        \"Problem,Characteristics\":[0,0,0],\n",
    "        \"Problem,Duration\":[0,0,0],\n",
    "        \"Problem,Frequency\":[0,0,0],\n",
    "        }\n",
    "        for file in glob(f'../dataset/**/predictions/**/*.csv'):\n",
    "            if exp in file and '_detailed' in file:\n",
    "                print(file)\n",
    "                lines=open(file).read().split('\\n')[1:]\n",
    "                out_ids=[]\n",
    "                for line in lines:\n",
    "                    if 'Problem,Trigger' in line and float(line.split(',')[-1])==0.5:\n",
    "                        out_ids.append(line.split(',')[0])\n",
    "                    \n",
    "                for line in lines:\n",
    "                    if line:\n",
    "                        words=line.split(',')[1:]\n",
    "                        key=','.join(words[:2])\n",
    "                        if key not in scores:\n",
    "                            #print(key)\n",
    "                            continue\n",
    "                        if any([id in line for id in out_ids]):\n",
    "                            #print(line)\n",
    "                            continue\n",
    "                        if words[3]!='0':\n",
    "                            for i in range(3):\n",
    "                                scores[key][i]+=int(words[i+3])\n",
    "        scores['Overall,-']=[0,0,0]\n",
    "        for i in range(3):\n",
    "            scores['Overall,-'][i]=sum([v[i] for k,v in scores.items()])\n",
    "\n",
    "        with open(f'../performance/entity_{exp}.csv','w') as f:\n",
    "            f.write('event,Argument,NT,NP,TP,P,R,F1\\n')\n",
    "            for key in scores:     \n",
    "                f.write(key+','+','.join([str(s) for s in scores[key]]))\n",
    "                P=scores[key][2]*100.0/scores[key][1] if scores[key][1] else 0\n",
    "                R=scores[key][2]*100.0/scores[key][0] if scores[key][0] else 0\n",
    "                F1=2*P*R/(P+R) if R+P else 0\n",
    "                f.write(f\",{round(P,1)},{round(R,1)},{round(F1,1)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from plmarker to spert json for evaluation\n",
    "from evaluate import *\n",
    "# for exp in['400window','sent']:\n",
    "#     source_file=f'../dataset/plmarker/test_{exp}.jsonl'\n",
    "#     exp=f'R15_re_{exp}'\n",
    "#     pred_ent_file=f'/home/velvinfu/code/PL-Marker/experiments/{exp}/ner_pred_results.json'\n",
    "#     pred_re_file=f'/home/velvinfu/code/PL-Marker/experiments/{exp}/re_pred_results.json'\n",
    "#     out_file=f'../dataset/plmarker/predictions/{exp}.json'\n",
    "#     plmarkerPred2json(source_file,pred_ent_file,pred_re_file,out_file)\n",
    "\n",
    "# relation from predicted entities\n",
    "source_file=f'../dataset/plmarker/predictions/pred_plmarker_400window.jsonl'\n",
    "exp='400window_preEnt'\n",
    "pred_ent_file=f'../dataset/plmarker/predictions/{exp}/ner_pred_results.json'\n",
    "pred_re_file=f'../dataset/plmarker/predictions/{exp}/re_pred_results.json'\n",
    "out_file=f'../dataset/plmarker/predictions/{exp}.json'\n",
    "plmarkerPred2json(source_file,pred_ent_file,pred_re_file,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE with QA format\n",
    "from evaluate import *\n",
    "spert_dir='../dataset/spert/test_spert_400window.json'#'../dataset/GenQA/predictions/t5_events_spert_400window.json'\n",
    "\n",
    "llama_input_dir='../dataset/GenQA/predictions/t5_events_re.json'\n",
    "llama_output_dir='../dataset/GenQA/predictions/t5_re_predEnt.json'\n",
    "outfile='../dataset/GenQA/predictions/t5_re_predEnt_spert.json'\n",
    "\n",
    "llama_input_dir='../dataset/GenQA/predictions/llama8b_events_re.json'\n",
    "llama_output_dir='../dataset/GenQA/predictions/llama8b_re_PredEnt.jsonl'\n",
    "outfile='../dataset/GenQA/predictions/llama8b_re_predEnt_spert.json'\n",
    "\n",
    "QARe2json(spert_dir,llama_input_dir,llama_output_dir,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE with the generative format\n",
    "from evaluate import *\n",
    "spert_dir='../dataset/spert/test_spert_400window.json'\n",
    "\n",
    "llama_input_dir=''\n",
    "llama_output_dir='../dataset/GenQA/predictions/t5_re_GenRe_pred.json'\n",
    "outfile='../dataset/GenQA/predictions/t5_re_GenRe_pred_spert.json'\n",
    "\n",
    "llama_input_dir=''\n",
    "llama_output_dir='../dataset/GenQA/predictions/gpt4_re_GenRe_pred.json'\n",
    "outfile='../dataset/GenQA/predictions/gpt4_re_GenRe_pred_spert.json'\n",
    "\n",
    "# llama_input_dir='../dataset/GenQA/test_Genre.json'\n",
    "# llama_output_dir='../dataset/GenQA/predictions/llama8b_re_GenRe_pred.jsonl'\n",
    "# outfile='../dataset/GenQA/predictions/llama8b_re_GenRe_pred_spert.json'\n",
    "\n",
    "# llama_input_dir=''\n",
    "# llama_output_dir='../dataset/GenQA/predictions/t5_re_GenRe_predEnt_pred.json'\n",
    "# outfile='../dataset/GenQA/predictions/t5_re_GenRe_predEnt_pred_spert.json'\n",
    "\n",
    "# llama_input_dir='../dataset/GenQA/predictions/llama8b_events_spert_GenRe.json'\n",
    "# llama_output_dir='../dataset/GenQA/predictions/llama8b_re_GenRe_predEnt_pred.jsonl'\n",
    "# outfile='../dataset/GenQA/predictions/llama8b_re_GenRe_predEnt_pred_spert.json'\n",
    "\n",
    "\n",
    "GenRe2json_predEnt(spert_dir,llama_input_dir,llama_output_dir,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import *\n",
    "source_file='../dataset/spert/test_spert_400window.json'\n",
    "\n",
    "models={\n",
    "    # 'spert':['re_400window'],\n",
    "    # 'plmarker':[\n",
    "    #             'R15_re_400window',\n",
    "    #             '400window_preEnt'\n",
    "    #             ],\n",
    "   'GenQA':[\n",
    "    #    't5_re_predEnt_spert',\n",
    "    #     'llama8b_re_predEnt_spert',\n",
    "    #     't5_re_GenRe_pred_spert',\n",
    "    #     'llama8b_re_GenRe_pred_spert',\n",
    "    #    't5_re_GenRe_predEnt_pred_spert',\n",
    "    #    'llama8b_re_GenRe_predEnt_pred_spert'\n",
    "        'gpt4_re_GenRe_pred_spert'\n",
    "   ]\n",
    "\n",
    "}\n",
    "\n",
    "for model,exps in models.items():\n",
    "    for exp in exps:\n",
    "        pred_file=f'../dataset/{model}/predictions/{exp}.json'\n",
    "        out_file=f'../performance/{model}_{exp}.csv'\n",
    "        error_file=f'../performance/{model}_{exp}_error.json'\n",
    "        score_relations(pred_file,source_file,out_file,error_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 966/10000 [00:00<00:06, 1291.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1672.80it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1822.49it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1819.12it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1821.93it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1804.39it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1819.69it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1787.23it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1824.94it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1819.86it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1826.76it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1829.17it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1821.40it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1828.00it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1831.11it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1835.62it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1819.71it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1821.07it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1824.27it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1829.41it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1828.71it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1832.39it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1832.85it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1829.68it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1831.79it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1836.93it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1841.30it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1842.37it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1837.93it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1841.57it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1839.50it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1837.22it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1840.10it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1839.33it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1833.78it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1840.28it/s]\n",
      "100%|██████████| 10000/10000 [00:05<00:00, 1839.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "def significance_test(ls1,ls2, num_iterations=10000):\n",
    "    p_values=[]\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        t1 = np.random.choice(ls1, size=len(ls1), replace=True)\n",
    "        t2 = np.random.choice(ls2, size=len(ls2), replace=True)\n",
    "        p_value=stats.ttest_ind(t1,t2,alternative='less').pvalue\n",
    "        p_values.append(p_value)\n",
    "    p_value=np.mean(p_values)\n",
    "    # if p_value<=0.05:\n",
    "    #     return p_value\n",
    "    return p_value\n",
    "\n",
    "filenames=glob('../dataset/BRAT/test/*.txt')\n",
    "filenames=[f.split('/')[-1].split('.')[0] for f in filenames]\n",
    "\n",
    "exps=['spert','plmarker','t5_events','llama2_events','llama8b_events','gpt4_events']\n",
    "\n",
    "keys=[\n",
    "    # '','Drug,Trigger','Problem,Trigger','Problem,Assertion','Problem,Change','Problem,Severity',\n",
    "    # 'Problem,Anatomy','Problem,Characteristics','Problem,Duration',\n",
    "    'Problem,Frequency'\n",
    "]\n",
    "\n",
    "def get_scores(key,file):\n",
    "    scores={}\n",
    "    for f in filenames:\n",
    "        scores[f]=[0,0,0]\n",
    "    for l in open(file).readlines()[1:]:\n",
    "        if l.strip() and ',' in l and key in l:\n",
    "            doc_id=l.split(',')[0]\n",
    "            for i in range(3):\n",
    "                scores[doc_id][i]+=int(l.split(',')[i+4])\n",
    "    \n",
    "    results=[]\n",
    "    for k,(NT,NP,TP) in scores.items():\n",
    "        P=TP/NP if NP else 0\n",
    "        R=TP/NT if NT else 0\n",
    "        F1=(P*R)*2/(P+R) if (P*R) else 0\n",
    "        results.append(F1)\n",
    "    return results\n",
    "\n",
    "candidate_files=glob(f'../dataset/**/predictions/**/*.csv')\n",
    "with open('../performance/ent_significance.csv','w') as f:\n",
    "    for key in keys:\n",
    "        f.write(f'\\n\\n{key}\\n,')\n",
    "        f.write(','.join([key for key in exps]))\n",
    "        for model1 in exps:\n",
    "            file1=[f for f in candidate_files if model1 in f and 'detailed' in f]\n",
    "            assert len(file1)==1\n",
    "            file1=file1[0]\n",
    "            f.write(f'\\n{model1},')\n",
    "            scores1=get_scores(key,file1)\n",
    "            for model2 in exps:\n",
    "                file2=[f for f in candidate_files if model2 in f and 'detailed' in f]\n",
    "                assert len(file2)==1,(model2,file2)\n",
    "                file2=file2[0]\n",
    "                scores2=get_scores(key,file2)\n",
    "                f.write(f'{significance_test(scores1,scores2)},')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def significance_test(ls1,ls2, num_iterations=10000):\n",
    "    p_values=[]\n",
    "    for i in range(num_iterations):\n",
    "        t1 = np.random.choice(ls1, size=len(ls1), replace=True)\n",
    "        t2 = np.random.choice(ls2, size=len(ls2), replace=True)\n",
    "        p_value=stats.ttest_ind(t1,t2,alternative='less').pvalue\n",
    "        p_values.append(p_value)\n",
    "    p_value=np.mean(p_values)\n",
    "    return p_value\n",
    "\n",
    "experiments={\n",
    "    'intra_gold':['plmarker_R15_re_400window_intra','GenQA_llama8b_re_GenRe_pred_spert_intra','llama8b_re_intra','t5_re_intra','GenQA_gpt4_re_GenRe_pred_spert_intra'],\n",
    "    # 'intra_pred':['spert_re_400window_intra','plmarker_400window_preEnt_intra',\n",
    "    #               'GenQA_llama8b_re_GenRe_predEnt_pred_spert_intra','GenQA_t5_re_GenRe_predEnt_pred_spert_intra',\n",
    "    #               'GenQA_t5_re_predEnt_spert_intra','GenQA_llama8b_re_predEnt_spert_intra'],\n",
    "    'all_gold':['plmarker_R15_re_400window_any','GenQA_llama8b_re_GenRe_pred_spert_any','t5_re_any','GenQA_gpt4_re_GenRe_pred_spert_intra'],\n",
    "    # 'all_pred':['spert_re_400window_intra','plmarker_400window_preEnt_any',\n",
    "    #             'GenQA_llama8b_re_GenRe_predEnt_pred_spert_any','GenQA_t5_re_GenRe_predEnt_pred_spert_any',\n",
    "    #   'GenQA_t5_re_predEnt_spert_any','GenQA_llama8b_re_predEnt_spert_any']\n",
    "}\n",
    "keys=['overall','AdminFor','NotAdminBecause','Causes','Improves','Worsens','PIP']\n",
    "\n",
    "def get_scores(key,file):\n",
    "    scores=[float(l.split(',')[-1]) for l in open(file).readlines()[1:] if l.strip() and ',' in l and (key in l or relation_names_reversed.get(key,'Overall') in l)]\n",
    "    return scores\n",
    "\n",
    "for title, exps in experiments.items():\n",
    "    with open(f'../performance/rel_significance_{title}.csv','w') as f:\n",
    "        for key in keys:\n",
    "            f.write(f'\\n\\n{key}\\n,')\n",
    "            f.write(','.join([key for key in exps]))\n",
    "            for model1 in exps:\n",
    "                file1=f'../performance/{model1}_by_docs.csv'\n",
    "                f.write(f'\\n{model1},')\n",
    "                scores1=get_scores(key,file1)\n",
    "                for model2 in exps:\n",
    "                    file2=f'../performance/{model2}_by_docs.csv'\n",
    "                    scores2=get_scores(key,file2)\n",
    "                    p_value=significance_test(scores1,scores2)\n",
    "                    f.write(f'{p_value},')\n",
    "                    #print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot performance vs distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "def get_trigger_scores(file):\n",
    "    scores={}\n",
    "    for f in filenames:\n",
    "        scores[f]=[0,0,0]\n",
    "    for l in open(file).readlines()[1:]:\n",
    "        if l.strip() and ',' in l and 'Trigger' in l:\n",
    "            doc_id=l.split(',')[0]\n",
    "            for i in range(3):\n",
    "                scores[doc_id][i]+=int(l.split(',')[i+4])\n",
    "    \n",
    "    results={}\n",
    "    for k,(NT,NP,TP) in scores.items():\n",
    "        P=TP/NP if NP else 0\n",
    "        R=TP/NT if NT else 0\n",
    "        F1=(P*R)*2/(P+R) if (P*R) else 0\n",
    "        results[k]=F1\n",
    "    return results\n",
    "\n",
    "# average number of triggers per 100 tokens\n",
    "filename='../dataset/spert/test_spert_note.json'\n",
    "source=json.loads(open(filename).read())\n",
    "count={}\n",
    "for dic in source:\n",
    "    triggers=len([ent for ent in dic['entities'] if 'Drug' in ent['type'] or 'Problem' in ent['type']])\n",
    "    count[dic['id']]=int(triggers*100.0/len(dic['tokens']))\n",
    "\n",
    "exps=['spert','plmarker','t5_events','llama8b_events','llama80b_events']\n",
    "\n",
    "candidate_files=glob(f'../dataset/**/predictions/**/*.csv')\n",
    "with open('../performance/ent_by_dis.csv','w') as f:\n",
    "    for i in range(4,9):\n",
    "        f.write(f',{i}')\n",
    "    for model in exps:\n",
    "        file=[f for f in candidate_files if model in f and 'detailed' in f]\n",
    "        assert len(file)==1\n",
    "        file=file[0]\n",
    "        f.write(f'\\n{model}')  \n",
    "        scores=get_trigger_scores(file)\n",
    "        for i in range(4,9):\n",
    "            score=np.mean([scores[f] for f,c in count.items() if c==i])\n",
    "            f.write(f',{score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
