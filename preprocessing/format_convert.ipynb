{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "from config import *\n",
    "from format_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spert: from BRAT to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brat .ann to json, note level.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "os.makedirs(f'../dataset/spert/',exist_ok=True)\n",
    "for split in ['train','valid','test']:\n",
    "    source_dir=f'../dataset/BRAT/{split}/*.ann'\n",
    "    outfile=f'../dataset/spert/{split}_spert_note.json'\n",
    "    brat2spert_note(source_dir,outfile,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. combining the assertion label to the trigger\n",
    "# 2. combining the subtype label to the label\n",
    "\n",
    "for split in ['train','valid','test']:\n",
    "    # source=f'../dataset/spert/{split}_spert.json'\n",
    "    # outfile=f'../dataset/spert/{split}_combined.json'\n",
    "    # combine_json_attributes(source,outfile)\n",
    "\n",
    "    source=f'../dataset/spert/{split}_spert_note.json'\n",
    "    outfile=f'../dataset/spert/{split}_note_combined.json'\n",
    "    combine_json_attributes(source,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.5.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 115/115 [00:48<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# spert json note-level into sliding window level.\n",
    "from format_helper import *\n",
    "window_size=400\n",
    "include_events=False\n",
    "for split in ['test']:#,'train','valid','test',\n",
    "    source_dir=f'../dataset/spert/{split}_note_combined.json'\n",
    "    outfile=f'../dataset/spert/{split}_spert_{window_size}window.json'\n",
    "    # output all chunks with all possible combinations of (start,end) sentences\n",
    "    # the relation's head and tail must be either the start and end sentence\n",
    "    # for the test, valid set, the head and tail entities cannot be in the same sentences, unless the chunk as only one sentence.\n",
    "    max_sent=4 if split!='test' else 1000\n",
    "    spertNote2spertWindow(split,source_dir,outfile,window_size,include_events,max_sent=max_sent)\n",
    "\n",
    "# split='train'\n",
    "# source_dir=f'../dataset/spert/{split}_note_combined.json'\n",
    "# outfile=f'../dataset/spert/{split}_spert_{window_size}window_complete.json'\n",
    "# # output all chunks with all possible combinations of (start,end) sentences\n",
    "# # the relation's head and tail must be either the start and end sentence\n",
    "# # for the test, valid set, the head and tail entities cannot be in the same sentences, unless the chunk as only one sentence.\n",
    "# spertNote2spertWindow(split,source_dir,outfile,window_size,include_events,include_entities_side=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spert json note-level into sentence_level.\n",
    "from format_helper import *\n",
    "for split in ['train','valid','test']:#\n",
    "    source_dir=f'../dataset/spert/{split}_note_combined.json'\n",
    "    outfile=f'../dataset/spert/{split}_spert_sent.json'\n",
    "    spertNote2spertSent(source_dir,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countting the relation statistics to make sure consistancy\n",
    "output=[key for key in relation_names]+['overall']\n",
    "for split in ['train','valid','test']:\n",
    "    counter={}\n",
    "    for key in relation_names:\n",
    "        counter[key]=0\n",
    "    counter['overall']=0\n",
    "    file=f'../dataset/spert/{split}_spert_400window.json'\n",
    "    data=json.loads(open(file).read())\n",
    "    for dic in data:\n",
    "        for rel in dic['relations']:\n",
    "            if rel['type'] in relation_names: # only intra-sentence relations\n",
    "                counter[rel['type']]+=1\n",
    "                counter['overall']+=1\n",
    "    for idx, key in enumerate(counter):\n",
    "        output[idx]+=f',{counter[key]}'\n",
    "    \n",
    "with open('../performance/relation_counts.csv','w') as f:\n",
    "    f.write('\\n'.join(output))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PL-Marker foramts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from format_helper import *\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "os.makedirs(f'../dataset/plmarker/',exist_ok=True)\n",
    "for split in ['valid','train','test']: #['train','valid','test']\n",
    "    for type in ['sent']:\n",
    "        source_file=f'../dataset/spert/{split}_spert_{type}.json'\n",
    "        output_filename=f'../dataset/plmarker/{split}_{type}.jsonl'\n",
    "        # setting it as none, because previous work has already counts the tokens.\n",
    "        entity_types,relation_types=spert2plmarker(source_file,output_filename,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.5.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# generative formats for events\n",
    "from format_helper import *\n",
    "os.makedirs(f'../dataset/GenQA/',exist_ok=True)\n",
    "for split in ['train','valid','test']:\n",
    "    source_file=f'../dataset/spert/{split}_spert_sent.json'\n",
    "    source_dir=f'../dataset/BRAT/{split}'\n",
    "    output_filename=f'../dataset/GenQA/{split}_events.json'\n",
    "    spert2GLM_evemts(source_file,source_dir,output_filename,split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.5.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# generative formats for RE\n",
    "from format_helper import *\n",
    "os.makedirs(f'../dataset/GenQA/',exist_ok=True)\n",
    "for split in ['train','valid','test']:#'train','valid','test'\n",
    "    source_file=f'../dataset/spert/{split}_spert_400window.json'\n",
    "    output_filename=f'../dataset/GenQA/{split}_Genre.json'\n",
    "    spert2T5_generativeRE(source_file,output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-QA formats for RE\n",
    "from format_helper import *\n",
    "os.makedirs(f'../dataset/GenQA/',exist_ok=True)\n",
    "for split in ['train','valid','test']:#'train','valid','test'\n",
    "    source_file=f'../dataset/spert/{split}_spert_400window.json'\n",
    "    output_filename=f'../dataset/GenQA/{split}_re.json'\n",
    "    neg_prob=0.2 if split=='train' else 1\n",
    "    spert2T5QA(source_file,output_filename,neg_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countting the relation statistics to make sure consistancy\n",
    "output=[key for key in relation_names]+['overall']\n",
    "for split in ['train','valid','test']:\n",
    "    counter={}\n",
    "    for key in relation_names:\n",
    "        counter[key]=0\n",
    "    counter['overall']=0\n",
    "    file=f'../dataset/GenQA/{split}_re.json'\n",
    "    data=json.loads(open(file).read())\n",
    "    for dic in data:\n",
    "        rel=dic['relation_source']\n",
    "        if rel in relation_names: # only intra-sentence relations\n",
    "            counter[rel]+=1\n",
    "            counter['overall']+=1\n",
    "    for idx, key in enumerate(counter):\n",
    "        output[idx]+=f',{counter[key]}'\n",
    "    \n",
    "with open('../performance/relation_counts.csv','w') as f:\n",
    "    f.write('\\n'.join(output))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM predicted entitites to RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brat .ann to json, note level.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from format_helper import *\n",
    "split='test'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "for exp in ['llama8b_events']:\n",
    "    source_dir=f'../dataset/GenQA/predictions/{exp}/pred/*.ann'\n",
    "    outfile=f'../dataset/GenQA/predictions/{exp}_spert_note.json'\n",
    "    brat2spert_note(source_dir,outfile,tokenizer)\n",
    "\n",
    "    source=outfile\n",
    "    outfile=f'../dataset/GenQA/predictions/{exp}_note_combined.json'\n",
    "    combine_json_attributes(source,outfile)\n",
    "\n",
    "    include_events=False\n",
    "    window_size=400\n",
    "    source_dir=outfile\n",
    "    outfile=f'../dataset/GenQA/predictions/{exp}_spert_{window_size}window.json'\n",
    "    spertNote2spertWindow(split,source_dir,outfile,window_size,include_events,max_sent=1000)\n",
    "\n",
    "    source_dir=f'../dataset/GenQA/predictions/{exp}_note_combined.json'\n",
    "    outfile=f'../dataset/GenQA/predictions/{exp}_spert_sent.json'\n",
    "    spertNote2spertSent(source_dir,outfile)\n",
    "\n",
    "    source_file=f'../dataset/GenQA/predictions/{exp}_spert_400window.json'\n",
    "    output_filename=f'../dataset/GenQA/predictions/{exp}_spert_GenRe.json'\n",
    "    spert2T5_generativeRE(source_file,output_filename)\n",
    "\n",
    "    source_file=f'../dataset/GenQA/predictions/{exp}_spert_400window.json'\n",
    "    output_filename=f'../dataset/GenQA/predictions/{exp}_re.json'\n",
    "    neg_prob=0.2 if split=='train' else 1\n",
    "    spert2T5QA(source_file,output_filename,neg_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PL-Marker predicted entity to RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from format_helper import *\n",
    "split='test'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "source_dir=f'../dataset/plmarker/predictions/BRAT/*.ann'\n",
    "outfile=f'../dataset/plmarker/predictions/pred_spert_note.json'\n",
    "#brat2spert_note(source_dir,outfile,tokenizer)\n",
    "\n",
    "source=outfile\n",
    "outfile=f'../dataset/plmarker/predictions/pred_note_combined.json'\n",
    "#combine_json_attributes(source,outfile)\n",
    "\n",
    "include_events=False\n",
    "window_size=400\n",
    "source_dir=outfile\n",
    "outfile=f'../dataset/plmarker/predictions/pred_spert_{window_size}window_unaligned.json'\n",
    "#spertNote2spertWindow(split,source_dir,outfile,window_size,include_events,max_sent=1000, tokenizer=tokenizer)\n",
    "\n",
    "# aligning the PL-Marker output to the original sentence. \n",
    "source_file=f'../dataset/plmarker/predictions/pred_spert_{window_size}window_unaligned.json'\n",
    "outfile=f'../dataset/plmarker/predictions/pred_spert_{window_size}window.json'\n",
    "reference_file='../dataset/spert/test_spert_400window.json'\n",
    "#align_spert_json(source_file,reference_file,outfile)\n",
    "\n",
    "\n",
    "source_file=f'../dataset/plmarker/predictions/pred_spert_{window_size}window.json'\n",
    "output_filename=f'../dataset/plmarker/predictions/pred_plmarker_{window_size}window.jsonl'\n",
    "# setting it as none, because previous work has already counts the tokens.\n",
    "entity_types,relation_types=spert2plmarker(source_file,output_filename,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file=f'../dataset/plmarker/predictions/pred_spert_{window_size}window_unaligned.json'\n",
    "outfile=f'../dataset/plmarker/predictions/pred_spert_{window_size}window.json'\n",
    "reference_file='../dataset/spert/test_spert_400window.json'\n",
    "\n",
    "def align_spert_json(source_file,reference_file,outfile):\n",
    "    pred=json.loads(open(source_file).read())\n",
    "    source=json.loads(open(reference_file).read())\n",
    "\n",
    "    for s in tqdm(source):\n",
    "        s['entities']=[]\n",
    "        s['relations']=[]\n",
    "        #s['tokens']=[t['text'] for t in s['tokens']]\n",
    "        for p in pred:\n",
    "            if p['id']==s['id']:\n",
    "                s['entities']=copy.deepcopy(p['entities'])\n",
    "                break\n",
    "\n",
    "    with open(outfile,'w') as f:\n",
    "        json.dump(source,f,indent=4)\n",
    "    return\n",
    "\n",
    "align_spert_json(source_file,reference_file,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=f'../dataset/plmarker/predictions/pred_spert_{window_size}window.json'\n",
    "source=json.loads(open(outfile).read())\n",
    "\n",
    "for s in tqdm(source):\n",
    "    s['tokens']=[t['text'] for t in s['tokens']]\n",
    "\n",
    "\n",
    "with open(outfile,'w') as f:\n",
    "    json.dump(source,f,indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
