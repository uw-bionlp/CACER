{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/thinc/compat.py:36: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  hasattr(torch, \"has_mps\")\n",
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/thinc/compat.py:37: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  and torch.has_mps  # type: ignore[attr-defined]\n",
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.5.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/velvinfu/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "from config import *\n",
    "from format_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spert: from BRAT to JSON\n",
    "1. sentence-level: ann2json.py\n",
    "2. sliding-window- and note- level: see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:25<00:00,  4.68it/s]\n",
      "100%|██████████| 60/60 [00:14<00:00,  4.13it/s]\n",
      "100%|██████████| 115/115 [00:26<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "os.makedirs(f'../dataset/spert/',exist_ok=True)\n",
    "for split in ['train','valid','test']:\n",
    "    source_dir=f'../dataset/BRAT/{split}/*.ann'\n",
    "    outfile=f'../dataset/spert/{split}_spert_note.json'\n",
    "    brat2spert_note(source_dir,outfile,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpERT: combining subtypes\n",
    "1. combining the assertion label to the trigger\n",
    "2. combining the subtype label to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'../dataset/spert/',exist_ok=True)\n",
    "for split in ['train','valid','test']:\n",
    "    source=f'../dataset/spert/{split}_spert.json'\n",
    "    outfile=f'../dataset/spert/{split}_combined.json'\n",
    "    combine_json_attributes(source,outfile)\n",
    "\n",
    "    source=f'../dataset/spert/{split}_spert_note.json'\n",
    "    outfile=f'../dataset/spert/{split}_note_combined.json'\n",
    "    combine_json_attributes(source,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting PL-Marker formats from Spert formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 notes converted from 32686 sentences.\n",
      "entities 18--------------------\n",
      "Anatomy\" ,\"Change-improving\" ,\"Change-no_change\" ,\"Change-resolved\" ,\"Change-worsening\" ,\"Characteristics\" ,\"Drug\" ,\"Duration\" ,\"Frequency\" ,\"Problem-absent\" ,\"Problem-conditional\" ,\"Problem-hypothetical\" ,\"Problem-not_patient\" ,\"Problem-possible\" ,\"Problem-present\" ,\"Severity-mild\" ,\"Severity-moderate\" ,\"Severity-severe\n",
      "relations 12--------------------\n",
      "PIP\" ,\"Problem-Anatomy\" ,\"Problem-Change\" ,\"Problem-Characteristics\" ,\"Problem-Duration\" ,\"Problem-Frequency\" ,\"Problem-Severity\" ,\"TrAP\" ,\"TrCP\" ,\"TrIP\" ,\"TrNAP\" ,\"TrWP\n",
      "60 notes converted from 5441 sentences.\n",
      "entities 18--------------------\n",
      "Anatomy\" ,\"Change-improving\" ,\"Change-no_change\" ,\"Change-resolved\" ,\"Change-worsening\" ,\"Characteristics\" ,\"Drug\" ,\"Duration\" ,\"Frequency\" ,\"Problem-absent\" ,\"Problem-conditional\" ,\"Problem-hypothetical\" ,\"Problem-not_patient\" ,\"Problem-possible\" ,\"Problem-present\" ,\"Severity-mild\" ,\"Severity-moderate\" ,\"Severity-severe\n",
      "relations 12--------------------\n",
      "PIP\" ,\"Problem-Anatomy\" ,\"Problem-Change\" ,\"Problem-Characteristics\" ,\"Problem-Duration\" ,\"Problem-Frequency\" ,\"Problem-Severity\" ,\"TrAP\" ,\"TrCP\" ,\"TrIP\" ,\"TrNAP\" ,\"TrWP\n",
      "115 notes converted from 10111 sentences.\n",
      "entities 18--------------------\n",
      "Anatomy\" ,\"Change-improving\" ,\"Change-no_change\" ,\"Change-resolved\" ,\"Change-worsening\" ,\"Characteristics\" ,\"Drug\" ,\"Duration\" ,\"Frequency\" ,\"Problem-absent\" ,\"Problem-conditional\" ,\"Problem-hypothetical\" ,\"Problem-not_patient\" ,\"Problem-possible\" ,\"Problem-present\" ,\"Severity-mild\" ,\"Severity-moderate\" ,\"Severity-severe\n",
      "relations 12--------------------\n",
      "PIP\" ,\"Problem-Anatomy\" ,\"Problem-Change\" ,\"Problem-Characteristics\" ,\"Problem-Duration\" ,\"Problem-Frequency\" ,\"Problem-Severity\" ,\"TrAP\" ,\"TrCP\" ,\"TrIP\" ,\"TrNAP\" ,\"TrWP\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "os.makedirs(f'../dataset/plmarker/',exist_ok=True)\n",
    "for split in ['train','valid','test']:\n",
    "    source_file=f'../dataset/spert/{split}_combined.json'\n",
    "    output_filename=f'../dataset/plmarker/{split}_plmarker.jsonl'\n",
    "    entity_types,relation_types=spert2plmarker(source_file,output_filename,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting the generating extraction formats from plmarker formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/401 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:01<00:00, 296.32it/s]\n",
      "100%|██████████| 61/61 [00:00<00:00, 244.01it/s]\n",
      "100%|██████████| 116/116 [00:00<00:00, 314.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# multi-QA formats for RE\n",
    "os.makedirs(f'../dataset/GenQA/',exist_ok=True)\n",
    "for split in ['train','valid','test']:\n",
    "    source_file=f'../dataset/plmarker/{split}_plmarker.jsonl'\n",
    "    output_filename=f'../dataset/GenQA/{split}_re.json'\n",
    "    plmarker2T5QA(source_file,output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
